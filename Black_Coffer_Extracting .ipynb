{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "830b1404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ualal\\AppData\\Local\\Temp\\ipykernel_51260\\4253058543.py:4: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "DRIVER_PATH = '/path/to/chromedriver'\n",
    "driver = webdriver.Chrome(executable_path=DRIVER_PATH)\n",
    "driver.get('https://google.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "298f5223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42</td>\n",
       "      <td>https://insights.blackcoffer.com/man-and-machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>43</td>\n",
       "      <td>https://insights.blackcoffer.com/in-future-or-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>45</td>\n",
       "      <td>https://insights.blackcoffer.com/how-machine-l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>46</td>\n",
       "      <td>https://insights.blackcoffer.com/deep-learning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>47</td>\n",
       "      <td>https://insights.blackcoffer.com/how-to-protec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0      37  https://insights.blackcoffer.com/ai-in-healthc...\n",
       "1      38  https://insights.blackcoffer.com/what-if-the-c...\n",
       "2      39  https://insights.blackcoffer.com/what-jobs-wil...\n",
       "3      40  https://insights.blackcoffer.com/will-machine-...\n",
       "4      41  https://insights.blackcoffer.com/will-ai-repla...\n",
       "5      42  https://insights.blackcoffer.com/man-and-machi...\n",
       "6      43  https://insights.blackcoffer.com/in-future-or-...\n",
       "7      45  https://insights.blackcoffer.com/how-machine-l...\n",
       "8      46  https://insights.blackcoffer.com/deep-learning...\n",
       "9      47  https://insights.blackcoffer.com/how-to-protec..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "URL = pd.read_csv('Book1.csv')\n",
    "URL.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21b5d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.remote.webdriver import WebDriver\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# read the Excel file\n",
    "df = pd.read_csv('Book1.csv')\n",
    "\n",
    "# create an empty list to store the extracted data\n",
    "data = []\n",
    "\n",
    "# loop over the links in the dataframe\n",
    "for link in df['URL']:\n",
    "    options = webdriver.Chrome()\n",
    "    options.get(link)\n",
    "    title = options.find_element(By.XPATH, '/html/body/div[6]/article/div[1]/div[1]/div[2]/div/header/h1').text\n",
    "    p_elements = options.find_elements(By.TAG_NAME, \"p\")\n",
    "    p_text = ''\n",
    "    for p in p_elements:\n",
    "        p_text = p_text + \"\\n\" + p.text\n",
    "    options.quit()\n",
    "\n",
    "    # add the extracted data to the list\n",
    "    data.append({'Title': title, 'Text': p_text})\n",
    "\n",
    "# create a new dataframe from the list\n",
    "df_data = pd.DataFrame(data)\n",
    "\n",
    "# write the data to a new Excel file or a CSV file\n",
    "df_data.to_excel('data.xlsx', index=False)\n",
    "# df_data.to_csv('data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c03f75ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed0867db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AI in healthcare to Improve Patient Outcomes</td>\n",
       "      <td>\\nIntroduction\\n“If anything kills over 10 mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What if the Creation is Taking Over the Creator?</td>\n",
       "      <td>\\nHuman minds, a fascination in itself carryin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What Jobs Will Robots Take From Humans in The ...</td>\n",
       "      <td>\\nIntroduction\\nAI is rapidly evolving in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Will Machine Replace The Human in the Future o...</td>\n",
       "      <td>\\n“Anything that could give rise to smarter-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Will AI Replace Us or Work With Us?</td>\n",
       "      <td>\\n“Machine intelligence is the last invention ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Blockchain for Payments</td>\n",
       "      <td>\\nReconciling with the financial realities of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>The future of Investing</td>\n",
       "      <td>\\nAn investment is a resource or thing procure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Big Data Analytics in Healthcare</td>\n",
       "      <td>\\nQuality and affordable healthcare is a visio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Business Analytics In The Healthcare Industry</td>\n",
       "      <td>\\nAnalytics is a statistical scientific proces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Challenges and Opportunities of Big Data in He...</td>\n",
       "      <td>\\nTo begin with I shall first like to explain ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0         AI in healthcare to Improve Patient Outcomes   \n",
       "1     What if the Creation is Taking Over the Creator?   \n",
       "2    What Jobs Will Robots Take From Humans in The ...   \n",
       "3    Will Machine Replace The Human in the Future o...   \n",
       "4                  Will AI Replace Us or Work With Us?   \n",
       "..                                                 ...   \n",
       "106                            Blockchain for Payments   \n",
       "107                            The future of Investing   \n",
       "108                   Big Data Analytics in Healthcare   \n",
       "109      Business Analytics In The Healthcare Industry   \n",
       "110  Challenges and Opportunities of Big Data in He...   \n",
       "\n",
       "                                                  Text  \n",
       "0    \\nIntroduction\\n“If anything kills over 10 mil...  \n",
       "1    \\nHuman minds, a fascination in itself carryin...  \n",
       "2    \\nIntroduction\\nAI is rapidly evolving in the ...  \n",
       "3    \\n“Anything that could give rise to smarter-th...  \n",
       "4    \\n“Machine intelligence is the last invention ...  \n",
       "..                                                 ...  \n",
       "106  \\nReconciling with the financial realities of ...  \n",
       "107  \\nAn investment is a resource or thing procure...  \n",
       "108  \\nQuality and affordable healthcare is a visio...  \n",
       "109  \\nAnalytics is a statistical scientific proces...  \n",
       "110  \\nTo begin with I shall first like to explain ...  \n",
       "\n",
       "[111 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_excel('data.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4bfbf2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-faced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-faces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abolish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abominable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4778</th>\n",
       "      <td>zaps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4779</th>\n",
       "      <td>zealot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4780</th>\n",
       "      <td>zealous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4781</th>\n",
       "      <td>zealously</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4782</th>\n",
       "      <td>zombie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4783 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "0        2-faced\n",
       "1        2-faces\n",
       "2       abnormal\n",
       "3        abolish\n",
       "4     abominable\n",
       "...          ...\n",
       "4778        zaps\n",
       "4779      zealot\n",
       "4780     zealous\n",
       "4781   zealously\n",
       "4782      zombie\n",
       "\n",
       "[4783 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_word=pd.read_csv(\"negative-words.txt\", sep='\\t', header=None, encoding='ISO-8859-1')\n",
    "negative_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e81969c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abundance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abundant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>youthful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>zeal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>zenith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>zest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>zippy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2006 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0            a+\n",
       "1        abound\n",
       "2       abounds\n",
       "3     abundance\n",
       "4      abundant\n",
       "...         ...\n",
       "2001   youthful\n",
       "2002       zeal\n",
       "2003     zenith\n",
       "2004       zest\n",
       "2005      zippy\n",
       "\n",
       "[2006 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_word= pd.read_csv(\"positive-words.txt\", sep='\\t', header=None, encoding='ISO-8859-1')\n",
    "positive_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a09e27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Words\n",
    "\n",
    "StopWords_Auditor= pd.read_csv(\"StopWords_Auditor.txt\", sep='\\t', header=None, encoding='ISO-8859-1')\n",
    "StopWords_Currencies= pd.read_csv(\"StopWords_Currencies.txt\", sep='\\t', header=None, encoding='ISO-8859-1')\n",
    "StopWords_DatesandNumbers= pd.read_csv(\"StopWords_DatesandNumbers.txt\", sep='\\t', header=None, encoding='ISO-8859-1')\n",
    "StopWords_Generic= pd.read_csv(\"StopWords_Generic.txt\", sep='\\t', header=None, encoding='ISO-8859-1')\n",
    "StopWords_GenericLong= pd.read_csv(\"StopWords_GenericLong.txt\", sep='\\t', header=None, encoding='ISO-8859-1')\n",
    "StopWords_Geographic= pd.read_csv(\"StopWords_Geographic.txt\", sep='\\t', header=None, encoding='ISO-8859-1')\n",
    "StopWords_Names= pd.read_csv(\"StopWords_Names.txt\", sep='\\t', header=None, encoding='ISO-8859-1')\n",
    "# StopWords_Names  \n",
    "# StopWords_Geographic \n",
    "# StopWords_GenericLong \n",
    "# StopWords_Generic\n",
    "# StopWords_DatesandNumbers  \n",
    "# StopWords_Currencies \n",
    "# StopWords_Auditor  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44cd29bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Read in the Excel file\n",
    "df = pd.read_excel(\"data.xlsx\")\n",
    "\n",
    "# Tokenize the \"Title\" and \"Text\" columns\n",
    "df[\"Title_Tokenized\"] = df[\"Title\"].apply(lambda x: word_tokenize(x))\n",
    "df[\"Text_Tokenized\"] = df[\"Text\"].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# Save the resulting dataframe as a new Excel file\n",
    "df.to_excel(\"tokenized_data.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62c56007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Title_Tokenized</th>\n",
       "      <th>Text_Tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AI in healthcare to Improve Patient Outcomes</td>\n",
       "      <td>\\nIntroduction\\n“If anything kills over 10 mil...</td>\n",
       "      <td>['AI', 'in', 'healthcare', 'to', 'Improve', 'P...</td>\n",
       "      <td>['Introduction', '“', 'If', 'anything', 'kills...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What if the Creation is Taking Over the Creator?</td>\n",
       "      <td>\\nHuman minds, a fascination in itself carryin...</td>\n",
       "      <td>['What', 'if', 'the', 'Creation', 'is', 'Takin...</td>\n",
       "      <td>['Human', 'minds', ',', 'a', 'fascination', 'i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What Jobs Will Robots Take From Humans in The ...</td>\n",
       "      <td>\\nIntroduction\\nAI is rapidly evolving in the ...</td>\n",
       "      <td>['What', 'Jobs', 'Will', 'Robots', 'Take', 'Fr...</td>\n",
       "      <td>['Introduction', 'AI', 'is', 'rapidly', 'evolv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Will Machine Replace The Human in the Future o...</td>\n",
       "      <td>\\n“Anything that could give rise to smarter-th...</td>\n",
       "      <td>['Will', 'Machine', 'Replace', 'The', 'Human',...</td>\n",
       "      <td>['“', 'Anything', 'that', 'could', 'give', 'ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Will AI Replace Us or Work With Us?</td>\n",
       "      <td>\\n“Machine intelligence is the last invention ...</td>\n",
       "      <td>['Will', 'AI', 'Replace', 'Us', 'or', 'Work', ...</td>\n",
       "      <td>['“', 'Machine', 'intelligence', 'is', 'the', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Blockchain for Payments</td>\n",
       "      <td>\\nReconciling with the financial realities of ...</td>\n",
       "      <td>['Blockchain', 'for', 'Payments']</td>\n",
       "      <td>['Reconciling', 'with', 'the', 'financial', 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>The future of Investing</td>\n",
       "      <td>\\nAn investment is a resource or thing procure...</td>\n",
       "      <td>['The', 'future', 'of', 'Investing']</td>\n",
       "      <td>['An', 'investment', 'is', 'a', 'resource', 'o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Big Data Analytics in Healthcare</td>\n",
       "      <td>\\nQuality and affordable healthcare is a visio...</td>\n",
       "      <td>['Big', 'Data', 'Analytics', 'in', 'Healthcare']</td>\n",
       "      <td>['Quality', 'and', 'affordable', 'healthcare',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Business Analytics In The Healthcare Industry</td>\n",
       "      <td>\\nAnalytics is a statistical scientific proces...</td>\n",
       "      <td>['Business', 'Analytics', 'In', 'The', 'Health...</td>\n",
       "      <td>['Analytics', 'is', 'a', 'statistical', 'scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Challenges and Opportunities of Big Data in He...</td>\n",
       "      <td>\\nTo begin with I shall first like to explain ...</td>\n",
       "      <td>['Challenges', 'and', 'Opportunities', 'of', '...</td>\n",
       "      <td>['To', 'begin', 'with', 'I', 'shall', 'first',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0         AI in healthcare to Improve Patient Outcomes   \n",
       "1     What if the Creation is Taking Over the Creator?   \n",
       "2    What Jobs Will Robots Take From Humans in The ...   \n",
       "3    Will Machine Replace The Human in the Future o...   \n",
       "4                  Will AI Replace Us or Work With Us?   \n",
       "..                                                 ...   \n",
       "106                            Blockchain for Payments   \n",
       "107                            The future of Investing   \n",
       "108                   Big Data Analytics in Healthcare   \n",
       "109      Business Analytics In The Healthcare Industry   \n",
       "110  Challenges and Opportunities of Big Data in He...   \n",
       "\n",
       "                                                  Text  \\\n",
       "0    \\nIntroduction\\n“If anything kills over 10 mil...   \n",
       "1    \\nHuman minds, a fascination in itself carryin...   \n",
       "2    \\nIntroduction\\nAI is rapidly evolving in the ...   \n",
       "3    \\n“Anything that could give rise to smarter-th...   \n",
       "4    \\n“Machine intelligence is the last invention ...   \n",
       "..                                                 ...   \n",
       "106  \\nReconciling with the financial realities of ...   \n",
       "107  \\nAn investment is a resource or thing procure...   \n",
       "108  \\nQuality and affordable healthcare is a visio...   \n",
       "109  \\nAnalytics is a statistical scientific proces...   \n",
       "110  \\nTo begin with I shall first like to explain ...   \n",
       "\n",
       "                                       Title_Tokenized  \\\n",
       "0    ['AI', 'in', 'healthcare', 'to', 'Improve', 'P...   \n",
       "1    ['What', 'if', 'the', 'Creation', 'is', 'Takin...   \n",
       "2    ['What', 'Jobs', 'Will', 'Robots', 'Take', 'Fr...   \n",
       "3    ['Will', 'Machine', 'Replace', 'The', 'Human',...   \n",
       "4    ['Will', 'AI', 'Replace', 'Us', 'or', 'Work', ...   \n",
       "..                                                 ...   \n",
       "106                  ['Blockchain', 'for', 'Payments']   \n",
       "107               ['The', 'future', 'of', 'Investing']   \n",
       "108   ['Big', 'Data', 'Analytics', 'in', 'Healthcare']   \n",
       "109  ['Business', 'Analytics', 'In', 'The', 'Health...   \n",
       "110  ['Challenges', 'and', 'Opportunities', 'of', '...   \n",
       "\n",
       "                                        Text_Tokenized  \n",
       "0    ['Introduction', '“', 'If', 'anything', 'kills...  \n",
       "1    ['Human', 'minds', ',', 'a', 'fascination', 'i...  \n",
       "2    ['Introduction', 'AI', 'is', 'rapidly', 'evolv...  \n",
       "3    ['“', 'Anything', 'that', 'could', 'give', 'ri...  \n",
       "4    ['“', 'Machine', 'intelligence', 'is', 'the', ...  \n",
       "..                                                 ...  \n",
       "106  ['Reconciling', 'with', 'the', 'financial', 'r...  \n",
       "107  ['An', 'investment', 'is', 'a', 'resource', 'o...  \n",
       "108  ['Quality', 'and', 'affordable', 'healthcare',...  \n",
       "109  ['Analytics', 'is', 'a', 'statistical', 'scien...  \n",
       "110  ['To', 'begin', 'with', 'I', 'shall', 'first',...  \n",
       "\n",
       "[111 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdf=pd.read_excel(\"tokenized_data.xlsx\")\n",
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "241d7cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Read in the tokenized data file\n",
    "df = pd.read_excel(\"tokenized_data.xlsx\")\n",
    "\n",
    "Read in the stop words files\n",
    "stop_words = set(stopwords.words())\n",
    "with open(\"StopWords_Auditor.txt\", \"r\") as f:\n",
    "    stop_words.update(f.read().splitlines())\n",
    "with open(\"StopWords_Currencies.txt\", \"r\") as f:\n",
    "    stop_words.update(f.read().splitlines())\n",
    "with open(\"StopWords_DatesandNumbers.txt\", \"r\") as f:\n",
    "    stop_words.update(f.read().splitlines())\n",
    "with open(\"StopWords_Generic.txt\", \"r\") as f:\n",
    "    stop_words.update(f.read().splitlines())\n",
    "with open(\"StopWords_GenericLong.txt\", \"r\") as f:\n",
    "    stop_words.update(f.read().splitlines())\n",
    "with open(\"StopWords_Geographic.txt\", \"r\") as f:\n",
    "    stop_words.update(f.read().splitlines())\n",
    "with open(\"StopWords_Names.txt\", \"r\") as f:\n",
    "    stop_words.update(f.read().splitlines())\n",
    "\n",
    "# # Remove stop words from \"Title_Tokenized\" and \"Text_Tokenized\" columns\n",
    "df[\"Title_Tokenized\"] = df[\"Title_Tokenized\"].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "df[\"Text_Tokenized\"] = df[\"Text_Tokenized\"].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "\n",
    "# # Save the resulting dataframe as a new Excel file\n",
    "df.to_excel(\"stopword_removed_data.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb8a34f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea4151a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DOING',\n",
       " 'DILLON',\n",
       " 'LEONARD',\n",
       " 'hočeta',\n",
       " 'ta',\n",
       " 'LINDEN',\n",
       " 'NINE',\n",
       " 'COOKS',\n",
       " 'MARIETTE',\n",
       " 'VERNETTA',\n",
       " '假若',\n",
       " 'VILLAGOMEZ',\n",
       " 'MELITA',\n",
       " 'tú',\n",
       " 'CECILE',\n",
       " '等',\n",
       " 'SHAE',\n",
       " 'sånn',\n",
       " 'smeta',\n",
       " 'eden',\n",
       " 'enakih',\n",
       " 'povrhu',\n",
       " 'ut',\n",
       " 'CRADDOCK',\n",
       " 'SEAN',\n",
       " 'PETRONILA',\n",
       " 'tivera',\n",
       " 'HANDY',\n",
       " 'tuon',\n",
       " 'LEVIN',\n",
       " 'pero',\n",
       " 'CINCINNATI',\n",
       " 'PEI',\n",
       " 'few',\n",
       " 'RANDY',\n",
       " 'CROWDER',\n",
       " 'तदनुसार',\n",
       " 'GILLMAN',\n",
       " 'eles',\n",
       " 'SARAN',\n",
       " 'MCEWEN',\n",
       " 'SHAQUANA',\n",
       " \"t'he\",\n",
       " 'নয়',\n",
       " '趁着',\n",
       " 'četrtimi',\n",
       " 'ثاء',\n",
       " 'theirs',\n",
       " 'гӯё',\n",
       " 'teillä',\n",
       " 'must',\n",
       " 'kakršnimkoli',\n",
       " 'takele',\n",
       " 'staremmo',\n",
       " 'كاد',\n",
       " 'PEGGY',\n",
       " '呼哧',\n",
       " 'REA',\n",
       " '此间',\n",
       " 'ingat-ingat',\n",
       " 'eso',\n",
       " 'AS',\n",
       " 'MARTINO',\n",
       " 'pela',\n",
       " 'jikalau',\n",
       " 'CORAL',\n",
       " 'sebuah',\n",
       " 'delarik',\n",
       " 'πως',\n",
       " 'esandakoaren',\n",
       " 'padahal',\n",
       " 'AM',\n",
       " 'SCOGGINS',\n",
       " 'ADELAIDA',\n",
       " 'тот',\n",
       " 'amelyet',\n",
       " '那里',\n",
       " 'sesekali',\n",
       " 'jidhar',\n",
       " 'CELINDA',\n",
       " 'WILLENE',\n",
       " 'catre',\n",
       " 'BROOME',\n",
       " 'stettero',\n",
       " 'SOSA',\n",
       " 'itulah',\n",
       " 'BOTT',\n",
       " 'njih',\n",
       " 'ALONSO',\n",
       " 'RUPEE  | India ',\n",
       " 'MARYLN',\n",
       " 'MATHERNE',\n",
       " 'dvakratne',\n",
       " 'SCHEER',\n",
       " 'WINGO',\n",
       " '前进',\n",
       " 'suoi',\n",
       " 'HOLLIMAN',\n",
       " 'PIRES',\n",
       " 'DANICA',\n",
       " 'GOODIN',\n",
       " 'second',\n",
       " 'tridesetemu',\n",
       " 'FINLAND',\n",
       " 'tuoksi',\n",
       " 'әйткенмен',\n",
       " 'олардан',\n",
       " 'ELISSA',\n",
       " 'YVONE',\n",
       " 'ғұрлы',\n",
       " 'nekoliko',\n",
       " 'SHUPE',\n",
       " 'OPAL',\n",
       " 'ROWLETT',\n",
       " 'DESROCHERS',\n",
       " 'VALLEY',\n",
       " '地',\n",
       " 'মধ্যভাগে',\n",
       " 'BEIJING',\n",
       " 'طاق',\n",
       " 'trinajsto',\n",
       " 'حتى',\n",
       " 'tenho',\n",
       " 'JULIET',\n",
       " 'SHOOK',\n",
       " 'werent',\n",
       " 'STEPHEN',\n",
       " 'JAMMIE',\n",
       " 'FLINT',\n",
       " 'MARQUEZ',\n",
       " 'MOORMAN',\n",
       " 'menunjukkan',\n",
       " 'VERONIKA',\n",
       " 'le-takem',\n",
       " 'kaunsa',\n",
       " '跟',\n",
       " 'BYARS',\n",
       " 'KIMBER',\n",
       " 'CONANT',\n",
       " 'le-takšnemu',\n",
       " 'أربع',\n",
       " 'PARKS',\n",
       " 'RANDELL',\n",
       " 'abbiano',\n",
       " 'س',\n",
       " 'CEDI | Ghana ',\n",
       " 'বললেন',\n",
       " 'ALEEN',\n",
       " 'BROWER',\n",
       " 'REYNOLDS',\n",
       " 'GARZA',\n",
       " 'khususnya',\n",
       " 'FALGOUST',\n",
       " 'SEEMA',\n",
       " 'JILL',\n",
       " 'CAPPS',\n",
       " 'جلل',\n",
       " 'यस्तो',\n",
       " 'oleh',\n",
       " 'غادر',\n",
       " 'то',\n",
       " 'تانِ',\n",
       " 'LAURINDA',\n",
       " 'KNOTT',\n",
       " 'KLINGER',\n",
       " 'true',\n",
       " 'HAMMONS',\n",
       " 'ELLEN',\n",
       " 'қабл',\n",
       " 'TIFFANEY',\n",
       " 'ALLYSON',\n",
       " 'περὶ',\n",
       " '当然',\n",
       " 'COSTON',\n",
       " 'আই',\n",
       " 'TYRRELL',\n",
       " 'DONOHUE',\n",
       " 'রাখা',\n",
       " 'MARIANA',\n",
       " 'הוא',\n",
       " '总的说来',\n",
       " 'DUPONT',\n",
       " 'HUNLEY',\n",
       " 'mintha',\n",
       " 'GOOLSBY',\n",
       " 'MIKA',\n",
       " 'berlangsung',\n",
       " 'SADLER',\n",
       " 'siden',\n",
       " 'GLADY',\n",
       " 'CORRIE',\n",
       " 'MURDOCK',\n",
       " 'CRABTREE',\n",
       " '故',\n",
       " 'izpred',\n",
       " 'MEDINA',\n",
       " 'STEELMAN',\n",
       " 'DORIS',\n",
       " 'aquestes',\n",
       " 'olisitte',\n",
       " 'VENEZUELA',\n",
       " 'KRATZ',\n",
       " 'لكيلا',\n",
       " 'ADRIA',\n",
       " 'MINDA',\n",
       " 'shouldnt',\n",
       " 'MARGERY',\n",
       " 'DEBBIE',\n",
       " 'også',\n",
       " 'TRUE',\n",
       " 'ERNEST',\n",
       " 'وإذ',\n",
       " 'BRUMMETT',\n",
       " 'ضاد',\n",
       " 'cîte',\n",
       " 'deset',\n",
       " 'MCKIBBEN',\n",
       " 'MAILE',\n",
       " 'STOTTS',\n",
       " 'KATHERINA',\n",
       " 'ARENA',\n",
       " 'TABOR',\n",
       " 'SILVEY',\n",
       " 'meine',\n",
       " 'naše',\n",
       " 'NUGENT',\n",
       " 'RODGER',\n",
       " 'COLBY',\n",
       " 'KARLYN',\n",
       " 'чтобы',\n",
       " 'HORNBACK',\n",
       " '无宁',\n",
       " 'যেখানে',\n",
       " 'whereafter',\n",
       " 'dena',\n",
       " 'FELTS',\n",
       " 'WRAY',\n",
       " 'LESA',\n",
       " 'vaš',\n",
       " 'TAMMY',\n",
       " 'NOLEN',\n",
       " 'BRENDA',\n",
       " 'JAQUELINE',\n",
       " 'SIXTEEN',\n",
       " 'jinho',\n",
       " '变成',\n",
       " 'LIDIA',\n",
       " 'lehetett',\n",
       " 'TREVINO',\n",
       " '继续',\n",
       " 'LEHR',\n",
       " 'hvad',\n",
       " 'SEBRINA',\n",
       " 'MCCLAIN',\n",
       " 'ANDREWS',\n",
       " 'SWINDELL',\n",
       " 'šestega',\n",
       " 'EVELYNE',\n",
       " 'сіздер',\n",
       " 'إذن',\n",
       " 'JUAN',\n",
       " 'jangankan',\n",
       " 'ستمائة',\n",
       " 'MAINE',\n",
       " 'LIBBIE',\n",
       " 'HENDON',\n",
       " 'аһа',\n",
       " 'ҳар',\n",
       " 'FENDER',\n",
       " 'ZELLER',\n",
       " 'BLEDSOE',\n",
       " 'VEST',\n",
       " 'ter',\n",
       " 'edinole',\n",
       " 'AUTRY',\n",
       " 'SELINA',\n",
       " 'JANSSEN',\n",
       " 'OLNEY',\n",
       " 'BERNARDA',\n",
       " 'CHARLTON',\n",
       " 'AFTON',\n",
       " 'CLIFTON',\n",
       " 'haven',\n",
       " 'ذوا',\n",
       " 'SAXTON',\n",
       " 'ثلاثمئة',\n",
       " 'KRESS',\n",
       " 'οὓς',\n",
       " '一切',\n",
       " '尔后',\n",
       " 'RILEY',\n",
       " 'MABELLE',\n",
       " 'JUDE',\n",
       " 'MEDLIN',\n",
       " 'HASKINS',\n",
       " 'JOHNS',\n",
       " 'GUENTHER',\n",
       " 'meihin',\n",
       " 'JEANETT',\n",
       " 'aaj',\n",
       " 'EAVES',\n",
       " 'OSWALT',\n",
       " 'prvima',\n",
       " 'CRITES',\n",
       " 'BOB',\n",
       " 'CARMINA',\n",
       " 'NANCEE',\n",
       " 'DONNIE',\n",
       " 'having',\n",
       " 'woh',\n",
       " 'MACKLIN',\n",
       " 'HITCHCOCK',\n",
       " 'LEIF',\n",
       " 'GULDEN | Netherlands Antilles ',\n",
       " 'LARI | Georgia ',\n",
       " 'SILVEIRA',\n",
       " 'enajst',\n",
       " 'COCHRAN',\n",
       " '适应',\n",
       " 'unhe',\n",
       " 'szinte',\n",
       " 'WAHL',\n",
       " 'bersama-sama',\n",
       " 'EVERETTE',\n",
       " 'HILDA',\n",
       " 'njihov',\n",
       " 'HU',\n",
       " '则',\n",
       " 'she',\n",
       " 'অবধি',\n",
       " 'HAUGEN',\n",
       " 'GANDY',\n",
       " 'CLEVENGER',\n",
       " 'tista',\n",
       " 'gaye',\n",
       " '毫不',\n",
       " 'BRATCHER',\n",
       " 'manakala',\n",
       " 'də',\n",
       " 'OHARA',\n",
       " 'JOSETTE',\n",
       " 'যথেষ্ট',\n",
       " 'DIEDRA',\n",
       " 'JEANNETTA',\n",
       " 'SERNA',\n",
       " 'ERIKA',\n",
       " 'tunjuk',\n",
       " 'أيّان',\n",
       " 'petnajsto',\n",
       " 'LILIANA',\n",
       " 'JASSO',\n",
       " 'LIVELY',\n",
       " 'HINKLE',\n",
       " 'ebbero',\n",
       " '您',\n",
       " 'LEANNE',\n",
       " 'गए',\n",
       " 'MOSES',\n",
       " 'HRYVNIA  | Ukraine ',\n",
       " 'SHENITA',\n",
       " 'mojih',\n",
       " 'BENALLY',\n",
       " 'BLYTHE',\n",
       " 'istima',\n",
       " 'MARCELENE',\n",
       " 'trojen',\n",
       " 'lehenengo',\n",
       " 'MCGOUGH',\n",
       " 'INGA',\n",
       " 'JO',\n",
       " 'kahaan',\n",
       " 'lenne',\n",
       " 'PURCELL',\n",
       " 'okay',\n",
       " 'COTTRELL',\n",
       " 'TENESHA',\n",
       " 'know',\n",
       " 'SHARIKA',\n",
       " 'өзім',\n",
       " '决定',\n",
       " 'তত',\n",
       " 'acestui',\n",
       " 'GROVER',\n",
       " 'CONAWAY',\n",
       " 'GAULT',\n",
       " 'KAITLIN',\n",
       " 'devetimi',\n",
       " '可以',\n",
       " 'zanj',\n",
       " 'CRITTENDEN',\n",
       " 'LEISA',\n",
       " 'তবু',\n",
       " 'NATALIE',\n",
       " 'PATTI',\n",
       " '常常',\n",
       " 'CHESTNUT',\n",
       " 'कि',\n",
       " 'міне',\n",
       " '它',\n",
       " 'תהיה',\n",
       " 'onu',\n",
       " 'PATE',\n",
       " '赶',\n",
       " 'hiç',\n",
       " 'CHIA',\n",
       " 'YUONNE',\n",
       " 'morate',\n",
       " 'DANLEY',\n",
       " 'ELLIE',\n",
       " 'bola',\n",
       " 'XVI',\n",
       " '今後',\n",
       " 'LINDA',\n",
       " 'WOODRUFF',\n",
       " 'shant',\n",
       " 'LARKIN',\n",
       " 'BONNETT',\n",
       " 'kakšnimi',\n",
       " 'SHANNA',\n",
       " 'FISH',\n",
       " 'asalkan',\n",
       " 'CULVER',\n",
       " 'žal',\n",
       " 'CINDERELLA',\n",
       " 'SADOWSKI',\n",
       " 'ZULMA',\n",
       " 'GOURDE | Haiti ',\n",
       " 'DUGGAN',\n",
       " 'našo',\n",
       " 'করেছেন',\n",
       " 'CAVAZOS',\n",
       " 'ASHLEIGH',\n",
       " 'ط',\n",
       " 'SARMIENTO',\n",
       " 'MEJIA',\n",
       " 'KNOWLTON',\n",
       " 'HENKEL',\n",
       " 'omdat',\n",
       " 'ole',\n",
       " 'SELLARS',\n",
       " 'SUMMERVILLE',\n",
       " 'PRIDDY',\n",
       " 'জন্য',\n",
       " 'vam',\n",
       " 'BERRYMAN',\n",
       " 'UN',\n",
       " 'অনেকেই',\n",
       " 'sudahlah',\n",
       " 'osemdesete',\n",
       " 'LATRICE',\n",
       " 'KATZ',\n",
       " 'COPELAND',\n",
       " 'FORTENBERRY',\n",
       " 'jedes',\n",
       " 'ayez',\n",
       " 'गरी',\n",
       " 'HUMMEL',\n",
       " '你',\n",
       " 'PELAYO',\n",
       " 'EGAN',\n",
       " 'ERDMAN',\n",
       " 'eine',\n",
       " 'ELA',\n",
       " '获得',\n",
       " 'UNRUH',\n",
       " 'yüz',\n",
       " 'STGERMAIN',\n",
       " 'enaindvajsete',\n",
       " 'FRIESEN',\n",
       " 'ثماني',\n",
       " 'sue',\n",
       " 'UTE',\n",
       " 'IRISH',\n",
       " 'YEVETTE',\n",
       " 'kakršni',\n",
       " 'METZLER',\n",
       " 'amelyekben',\n",
       " 'demikian',\n",
       " 'KILLIAN',\n",
       " 'LINCOLN',\n",
       " 'demi',\n",
       " 'три',\n",
       " 'wish',\n",
       " 'এখানে',\n",
       " 'nekakimi',\n",
       " 'EDELMAN',\n",
       " 'najbrž',\n",
       " 'BRADLY',\n",
       " 'rakho',\n",
       " 'LADAWN',\n",
       " 'ROSALES',\n",
       " 'petinosemdesetih',\n",
       " 'EIGHTH',\n",
       " 'LIESELOTTE',\n",
       " 'BONE',\n",
       " 'aldiz ',\n",
       " 'HARPER',\n",
       " 'DOLLY',\n",
       " 'dimaksudkan',\n",
       " 'JAYNA',\n",
       " 'ὡς',\n",
       " 'COFIELD',\n",
       " 'CRUTCHFIELD',\n",
       " 'SEVILLA',\n",
       " 'FARKAS',\n",
       " 'ji',\n",
       " 'AMI',\n",
       " 'JANELLE',\n",
       " 'sebabnya',\n",
       " 'hun',\n",
       " 'bila',\n",
       " 'METTS',\n",
       " 'BANNON',\n",
       " 'NAIDA',\n",
       " 'POLLY',\n",
       " 'агар',\n",
       " 'HILDEGARDE',\n",
       " 'nekakem',\n",
       " 'he',\n",
       " 'hvorfor',\n",
       " 'MOIRA',\n",
       " 'için',\n",
       " 'jawabnya',\n",
       " 'LACROIX',\n",
       " 'ALTHEA',\n",
       " 'ANSLEY',\n",
       " 'SLAUGHTER',\n",
       " 'DAWSON',\n",
       " 'van',\n",
       " 'BERNADINE',\n",
       " 'nikakršnim',\n",
       " 'ισωσ',\n",
       " 'OVER',\n",
       " 'tenim',\n",
       " 'গেল',\n",
       " 'SALE',\n",
       " 'LINEBERRY',\n",
       " 'EBONI',\n",
       " 'FARNSWORTH',\n",
       " 'LOCKLEAR',\n",
       " 'menanti-nanti',\n",
       " 'TESS',\n",
       " 'إليك',\n",
       " 'CHIN',\n",
       " '一直',\n",
       " 'želi',\n",
       " 'кә',\n",
       " '己',\n",
       " 'tuoi',\n",
       " 'WESTBROOK',\n",
       " '这么些',\n",
       " 'diperlihatkan',\n",
       " 'GIVENS',\n",
       " 'NEIGHBORS',\n",
       " 'PARKE',\n",
       " 'osemdesetimi',\n",
       " 'MICHAELS',\n",
       " 'selamanya',\n",
       " 'SHOSHANA',\n",
       " 'دونك',\n",
       " 'FARIA',\n",
       " 'FUDGE',\n",
       " 'THOMASINA',\n",
       " 'tolike',\n",
       " 'أنّى',\n",
       " 'е',\n",
       " 'osmem',\n",
       " 'أخو',\n",
       " 'MARLEN',\n",
       " 'PERRY',\n",
       " 'تلك',\n",
       " 'رويدك',\n",
       " 'DORTHA',\n",
       " 'JACQUETTA',\n",
       " 'قبل',\n",
       " 'BRANTLEY',\n",
       " 'HEADLEY',\n",
       " 'RIEDEL',\n",
       " 'BURKHOLDER',\n",
       " 'вой-вой',\n",
       " 'BISH',\n",
       " 'MARCHANT',\n",
       " 'FLYNN',\n",
       " 'ADRIAN',\n",
       " 'أول',\n",
       " 'VERN',\n",
       " 'حبيب',\n",
       " 'CANDANCE',\n",
       " 'GRIBBLE',\n",
       " 'CHUN',\n",
       " 'bagaimanapun',\n",
       " 'REEVE',\n",
       " 'қаңқ-қаңқ',\n",
       " 'ELLI',\n",
       " 'WOODMAN',\n",
       " 'FLORETTA',\n",
       " 'فيما',\n",
       " 'RUTH',\n",
       " 'CASH',\n",
       " 'CASTELLANOS',\n",
       " 'RANDLE',\n",
       " 'BLOSSOM',\n",
       " 'elõ',\n",
       " 'MEYER',\n",
       " 'sekurangnya',\n",
       " 'UNDERWOOD',\n",
       " 'CLEM',\n",
       " 'DARWIN',\n",
       " 'FREDDA',\n",
       " 'čigavima',\n",
       " 'CLOUD',\n",
       " 'kemungkinan',\n",
       " 'MARIANO',\n",
       " 'FERMIN',\n",
       " 'GARRIS',\n",
       " 'SACHA',\n",
       " 'MCCANDLESS',\n",
       " 'KEENA',\n",
       " 'WENONA',\n",
       " 'MODESTO',\n",
       " 'heihin',\n",
       " 'MANN',\n",
       " 'THOMSON',\n",
       " 'habíais',\n",
       " 'TORRENCE',\n",
       " 'ALAINE',\n",
       " 'SEEGER',\n",
       " 'SILVA',\n",
       " 'enggaknya',\n",
       " 'KARIMA',\n",
       " 'ба шарте',\n",
       " 'ISMAEL',\n",
       " 'ADOLPH',\n",
       " 'AMEE',\n",
       " 'TSE',\n",
       " 'SEARLES',\n",
       " 'оё',\n",
       " 'sendirian',\n",
       " 'AMATO',\n",
       " 'CECILLE',\n",
       " 'andar',\n",
       " 'ROSALEE',\n",
       " 'ALBRIGHT',\n",
       " 'stavi',\n",
       " 'četrto',\n",
       " 'trikratna',\n",
       " 'SHAN',\n",
       " 'نيف',\n",
       " 'RUBINO',\n",
       " 'HATLEY',\n",
       " '呕',\n",
       " 'KATIE',\n",
       " 'loro',\n",
       " 'أخذ',\n",
       " 'إلَيْكَ',\n",
       " 'TREECE',\n",
       " 'osmima',\n",
       " 'blei',\n",
       " 'ভাবে',\n",
       " 'zmoreta',\n",
       " 'häntä',\n",
       " 'CARTAGENA',\n",
       " 'indem',\n",
       " 'AVIS',\n",
       " 'MANDA',\n",
       " 'BLAYLOCK',\n",
       " 'mindenki',\n",
       " 'לעיכן',\n",
       " 'RAYMOND',\n",
       " 'SALENA',\n",
       " 'GRIFFIN',\n",
       " 'étaient',\n",
       " 'VERLINE',\n",
       " 'saatnya',\n",
       " 'štiriindvajsetega',\n",
       " 'sekä',\n",
       " 'semisal',\n",
       " 'جوان',\n",
       " 'LYNELLE',\n",
       " 'KINNEY',\n",
       " 'WEED',\n",
       " 'DAINA',\n",
       " 'le-takšen',\n",
       " 'LOCKARD',\n",
       " 'eûmes',\n",
       " '由于',\n",
       " 'SOMERS',\n",
       " 'krog',\n",
       " 'DELPHINE',\n",
       " 'namreč',\n",
       " 'ESQUIVEL',\n",
       " 'DID',\n",
       " 'ILUMINADA',\n",
       " 'THROUGH',\n",
       " 'ف',\n",
       " 'honetan',\n",
       " 'baje',\n",
       " 'petindvajsetima',\n",
       " 'ERICKSON',\n",
       " 'BURDETT',\n",
       " 'RAGAN',\n",
       " 'HURT',\n",
       " '凭',\n",
       " 'LUISA',\n",
       " 'kann',\n",
       " 'DUNHAM',\n",
       " 'SHARP',\n",
       " 'BARHAM',\n",
       " 'JOHN',\n",
       " 'idi',\n",
       " 'míg',\n",
       " 'RAYLENE',\n",
       " 'KIERSTEN',\n",
       " 'ANGEL',\n",
       " 'tisočere',\n",
       " 'HAMMETT',\n",
       " 'sejauh',\n",
       " 'BIDDLE',\n",
       " 'dela',\n",
       " 'smete',\n",
       " 'SHRADER',\n",
       " 'DANELLE',\n",
       " 'smel',\n",
       " 'btw',\n",
       " 'MEDLOCK',\n",
       " 'HERNANDES',\n",
       " 'мыңқ',\n",
       " 'SMOCK',\n",
       " 'tristoti',\n",
       " 'SHANKLIN',\n",
       " 'memulai',\n",
       " 'أهلا',\n",
       " 'ASBURY',\n",
       " 'berlebihan',\n",
       " 'somme',\n",
       " 'DARLEEN',\n",
       " 'aviez',\n",
       " 'GIGI',\n",
       " 'misal',\n",
       " 'ANIBAL',\n",
       " 'JOELLA',\n",
       " 'LYNELL',\n",
       " 'KRISTA',\n",
       " 'DEVORE',\n",
       " 'ته',\n",
       " 'ROWAN',\n",
       " 'ans',\n",
       " 'CARRY',\n",
       " 'saranno',\n",
       " 'BETHANN',\n",
       " 'ممن',\n",
       " 'JASMIN',\n",
       " 'ὅστις',\n",
       " 'ой',\n",
       " 'WALTZ',\n",
       " 'ELLAN',\n",
       " 'serán',\n",
       " 'WIEGAND',\n",
       " 'MIKEL',\n",
       " 'أولاء',\n",
       " 'seríais',\n",
       " 'JONAS',\n",
       " 'NADENE',\n",
       " 'lehet',\n",
       " 'TROUPE',\n",
       " 'WITHERS',\n",
       " 'cîţi',\n",
       " 'isnt',\n",
       " 'SVETLANA',\n",
       " 'বসে',\n",
       " 'sitta',\n",
       " 'temuintemu',\n",
       " 'зеро',\n",
       " 'WHITWORTH',\n",
       " '倘若',\n",
       " 'corresponding',\n",
       " 'FORESTER',\n",
       " 'bé',\n",
       " '今天',\n",
       " 'MCKINNON',\n",
       " '不变',\n",
       " 'else',\n",
       " 'NGOC',\n",
       " 'dintr-',\n",
       " 'ALLENE',\n",
       " 'got',\n",
       " 'seves',\n",
       " 'जबकि',\n",
       " 'ISBELL',\n",
       " 'SHEPHERD',\n",
       " 'CASPER',\n",
       " 'PENDLETON',\n",
       " 'ELINORE',\n",
       " 'সঙ্গে',\n",
       " 'NAOMI',\n",
       " 'SHARELL',\n",
       " 'tretjega',\n",
       " 'HAWKES',\n",
       " 'GUERTIN',\n",
       " 'hoče',\n",
       " 'هللة',\n",
       " 'LALLY',\n",
       " 'জনকে',\n",
       " 'PORRAS',\n",
       " 'ROD',\n",
       " 'এমনি',\n",
       " 'ERWIN',\n",
       " 'VELVA',\n",
       " 'dovolile',\n",
       " 'DICARLO',\n",
       " 'FARBER',\n",
       " 'against',\n",
       " 'vsakršni',\n",
       " 'KILPATRICK',\n",
       " 'HESSE',\n",
       " 'kinilah',\n",
       " 'STEPHINE',\n",
       " 'aurez',\n",
       " 'GERRY',\n",
       " 'SEALE',\n",
       " 'тек',\n",
       " 'OCEAN',\n",
       " 'STUBBLEFIELD',\n",
       " 'zares',\n",
       " 'MONDAY',\n",
       " 'became',\n",
       " 'CRISTINA',\n",
       " 'MCCANN',\n",
       " 'EMA',\n",
       " 'aitzitik',\n",
       " 'ROYER',\n",
       " 'عَدَسْ',\n",
       " 'VICTOR',\n",
       " 'FREDERICKS',\n",
       " 'DORATHY',\n",
       " 'SPRADLIN',\n",
       " 'DAHL',\n",
       " 'MAYERS',\n",
       " 'ROLON',\n",
       " 'SWAFFORD',\n",
       " 'g',\n",
       " 'STACEE',\n",
       " 'BIRMINGHAM',\n",
       " 'BETHEL',\n",
       " 'we',\n",
       " 'MCADOO',\n",
       " '随著',\n",
       " 'cuma',\n",
       " 'LUMPKIN',\n",
       " 'PAQUETTE',\n",
       " 'JEANE',\n",
       " 'jisme',\n",
       " 'vsako',\n",
       " 'JEREMIAH',\n",
       " '哎呀',\n",
       " 'PENTON',\n",
       " 'tvojemu',\n",
       " 'SCOTT',\n",
       " 'DOLL',\n",
       " 'тем',\n",
       " 'WINDY',\n",
       " 'ROBINETTE',\n",
       " 'δαίσ',\n",
       " '假如',\n",
       " 'CHER',\n",
       " 'CAPONE',\n",
       " 'ST',\n",
       " 'ANDRUS',\n",
       " 'FRITH',\n",
       " 'DREYER',\n",
       " 'negli',\n",
       " 'CHANEL',\n",
       " 'uss',\n",
       " 'diesen',\n",
       " 'WINNIFRED',\n",
       " '叫做',\n",
       " 'BEAL',\n",
       " 'LAMONT',\n",
       " 'ati',\n",
       " 'FORDHAM',\n",
       " 'FRYE',\n",
       " 'ZACKARY',\n",
       " 'vort',\n",
       " 'CURTISS',\n",
       " 'MAREN',\n",
       " 'কোটি',\n",
       " 'BOULDIN',\n",
       " 'сенің',\n",
       " 'suyo',\n",
       " 'sekalian',\n",
       " '啊',\n",
       " 'REINA',\n",
       " 'még',\n",
       " 'devetem',\n",
       " 'со',\n",
       " 'hone',\n",
       " 'kolikšnem',\n",
       " 'AIMEE',\n",
       " 'OBRIEN',\n",
       " 'WOOTEN',\n",
       " 'MICHELL',\n",
       " 'RUFINA',\n",
       " 'teh',\n",
       " 'KACY',\n",
       " 'SHANA',\n",
       " 'HERBERT',\n",
       " 'BRADDOCK',\n",
       " 'selv',\n",
       " 'bəy',\n",
       " 'DORCAS',\n",
       " 'JENINE',\n",
       " 'düz',\n",
       " 'COTTER',\n",
       " 'ELZA',\n",
       " 'VIVIEN',\n",
       " 'कुरा',\n",
       " 'placed',\n",
       " '什么样',\n",
       " 'WILT',\n",
       " 'CARPER',\n",
       " '那些',\n",
       " 'bestela',\n",
       " 'ثمة',\n",
       " 'STULL',\n",
       " 're',\n",
       " 'HILLMAN',\n",
       " 'WESTOVER',\n",
       " 'LEVERETT',\n",
       " 'JERRI',\n",
       " 'سابع',\n",
       " 'PHILLIPS',\n",
       " 'KARI',\n",
       " 'LANKFORD',\n",
       " 'želela',\n",
       " 'ὅπερ',\n",
       " 'কারণ',\n",
       " 'LELA',\n",
       " 'рӯacaba',\n",
       " 'LAINE',\n",
       " 'MILDA',\n",
       " 'CARLOS',\n",
       " 'sedemdesetem',\n",
       " 'primer',\n",
       " 'QUINONES',\n",
       " 'HUNTINGTON',\n",
       " 'marajte',\n",
       " 'hadn',\n",
       " 'hanem',\n",
       " 'BRIM',\n",
       " 'ULLOA',\n",
       " 'wasnt',\n",
       " 'DAILY',\n",
       " 'estar',\n",
       " 'BERMUDA',\n",
       " 'mele',\n",
       " 'banae',\n",
       " 'little',\n",
       " 'NIDA',\n",
       " '有着',\n",
       " 'NEILL',\n",
       " 'MEE',\n",
       " 'PENNIE',\n",
       " 'é',\n",
       " 'štirih',\n",
       " 'MADALINE',\n",
       " 'KEELING',\n",
       " 'nope',\n",
       " 'tätä',\n",
       " 'QUEZADA',\n",
       " 'DELORA',\n",
       " 'BOOTHE',\n",
       " 'CARLETTA',\n",
       " 'OSCAR',\n",
       " 'CLINE',\n",
       " 'DIPIETRO',\n",
       " 'FLICK',\n",
       " 'تلقاء',\n",
       " 'keillä',\n",
       " 'begitukah',\n",
       " 'katerimakoli',\n",
       " 'altre',\n",
       " 'WELLER',\n",
       " 'бо вуҷуди он ки',\n",
       " 'ons',\n",
       " 'hortik',\n",
       " 'CANTRELL',\n",
       " 'EVELINA',\n",
       " 'BONNEY',\n",
       " 'FLETCHER',\n",
       " '突然',\n",
       " 'MCDUFFIE',\n",
       " 'REINHART',\n",
       " 'BATCHELOR',\n",
       " 'LANNY',\n",
       " 'MCCABE',\n",
       " 'GALLO',\n",
       " 'BRIGITTE',\n",
       " 'ROOD',\n",
       " 'ποιοσ',\n",
       " 'COWEN',\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
